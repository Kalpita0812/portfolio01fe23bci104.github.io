<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Algorithms and Data Structures</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
        }
        h1, h2 {
            color: #2c3e50;
        }
        h3 {
            color: #34495e;
        }
        p, li {
            font-size: 16px;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        .algorithm-title {
            font-weight: bold;
            color: #2980b9;
        }
    </style>
</head>
<body>
    <h1>Algorithms and Data Structures</h1>
    
    <h2>Space and Time Efficiency</h2>
    <p><strong>Importance of Analyzing Algorithmic Complexity</strong></p>
    <p>Analyzing algorithm complexity ensures scalability and efficient handling of larger data. For example, bubble sort has <span class="algorithm-title">O(n²)</span>, while merge sort has <span class="algorithm-title">O(n log n)</span>.</p>
    
    <h3>Orders of Growth</h3>
    <ul>
        <li><strong>O(1)</strong>: Constant time (e.g., array access)</li>
        <li><strong>O(log n)</strong>: Logarithmic time (e.g., binary search)</li>
        <li><strong>O(n)</strong>: Linear time (e.g., iterating through an array)</li>
        <li><strong>O(n²)</strong>: Quadratic time (e.g., bubble sort)</li>
    </ul>

    <h2>Sorting and Searching</h2>

    <h3>Sorting Algorithms</h3>
    
    <p><strong>Insertion Sort:</strong> Builds the final sorted list one item at a time by inserting elements into their correct positions.</p>
    <p><strong>Time Complexity:</strong> O(n²) in the average and worst case, O(n) in the best case.</p>

    <p><strong>Selection Sort:</strong> Repeatedly finds the minimum element from the unsorted part and moves it to the sorted part.</p>
    <p><strong>Time Complexity:</strong> O(n²) in both average and worst case.</p>

    <p><strong>Heap Sort:</strong> Converts the list into a heap and repeatedly extracts the maximum or minimum element, rebuilding the heap after each extraction.</p>
    <p><strong>Time Complexity:</strong> O(n log n) in the best, average, and worst cases.</p>

    <p><strong>Bubble Sort:</strong></p>
    <pre class="algorithm-title">
ALGORITHM BubbleSort(A[0..n-1])
    for i <- 0 to n - 2 do
        for j <- 0 to n - 2 - i do
            if A[j+1] < A[j]
                swap A[j] and A[j+1]
    </pre>

    <p><strong>Merge Sort:</strong> Efficient divide-and-conquer sorting algorithm with O(n log n) time complexity.</p>
    <pre class="algorithm-title">
ALGORITHM MergeSort(A[0..n-1])
    if n > 1
        ...
    </pre>

    <p><strong>Quick Sort:</strong> Another divide-and-conquer algorithm that selects a pivot and partitions the array.</p>
    <pre class="algorithm-title">
#include <iostream>
...
    </pre>

    <h3>Searching Algorithms</h3>
    <p><strong>Binary Search:</strong> Binary search is an efficient algorithm for finding an element in a sorted array with O(log n) time complexity.</p>
    <pre class="algorithm-title">
int binarySearch(const vector<int>& arr, int key) {
    ...
}
    </pre>

    <h2>Graph Algorithms</h2>

    <h3>Spanning Trees</h3>
    <p><strong>Prim's Algorithm:</strong> Finds the Minimum Spanning Tree (MST) by starting with a single vertex and growing the tree by adding the shortest edge connecting to an outside vertex.</p>
    <p><strong>Time Complexity:</strong> O(E log V) using a priority queue, where E is the number of edges and V is the number of vertices.</p>

    <p><strong>Kruskal’s Algorithm:</strong></p>
    <pre class="algorithm-title">
int Find(int parent[], int i) {
    ...
}
    </pre>

    <h3>Shortest Path Algorithms</h3>

    <p><strong>Dijkstra's Algorithm:</strong> Finds the shortest path between two points in a graph.</p>
    <pre class="algorithm-title">
#include <iostream>
...
    </pre>

    <p><strong>Bellman-Ford Algorithm:</strong> Computes shortest paths from a single source vertex, handling graphs with negative weights.</p>
    <p><strong>Time Complexity:</strong> O(VE), where V is the number of vertices and E is the number of edges.</p>

    <p><strong>Floyd-Warshall Algorithm:</strong> A dynamic programming algorithm for finding shortest paths between all pairs of vertices.</p>
    <p><strong>Time Complexity:</strong> O(V³), where V is the number of vertices.</p>

    <h3>Traversal Algorithms</h3>
    <p><strong>Breadth-First Search (BFS):</strong> Explores nodes layer by layer, visiting all neighbors before moving deeper.</p>
    <p><strong>Depth-First Search (DFS):</strong> Explores as far as possible along each branch before backtracking.</p>

    <h3>Comparison of BFS and DFS:</h3>
    <ul>
        <li><strong>BFS:</strong> Uses more memory due to the queue and guarantees the shortest path in unweighted graphs.</li>
        <li><strong>DFS:</strong> Uses less memory due to the stack (or recursion) and can be more efficient in scenarios like puzzles.</li>
    </ul>

    <h2>Pattern Searching Algorithms</h2>

    <p><strong>Brute Force (Naive Approach):</strong> Checks for the pattern at every possible position in the text.</p>
    <p><strong>Time Complexity:</strong> O((n - m + 1) * m), where n is the length of the text and m is the length of the pattern.</p>

    <p><strong>Knuth-Morris-Pratt (KMP) Algorithm:</strong> Preprocesses the pattern to skip unnecessary comparisons.</p>
    <p><strong>Time Complexity:</strong> O(n + m).</p>

    <p><strong>Boyer-Moore Algorithm:</strong> Uses heuristics to skip sections of the text, improving search speed.</p>
    <p><strong>Time Complexity:</strong> Best case O(n/m); worst case O(n * m).</p>

    <p><strong>Rabin-Karp Algorithm:</strong> Uses hashing to search for patterns efficiently.</p>
    <p><strong>Time Complexity:</strong> Average case O(n + m); worst case O(n * m) due to hash collisions.</p>

    <h2>Algorithm Design Techniques</h2>
    <ul>
        <li><strong>Divide and Conquer:</strong> Breaks problems into smaller sub-problems.</li>
        <li><strong>Greedy Algorithms:</strong> Makes the best choice at each step.</li>
        <li><strong>Dynamic Programming:</strong> Solves problems by storing results of overlapping subproblems.</li>
    </ul>

    <h2>Reflections</h2>
    <p>Learning how to break down complex problems into smaller components allows for better problem-solving and optimization. Balancing optimization with simplicity ensures maintainability while delivering efficient solutions. Adapting solutions across different challenges involves choosing the right algorithm for the problem at hand.</p>

    <a href="index.html">Back to Portfolio</a>
</body>
</html>
