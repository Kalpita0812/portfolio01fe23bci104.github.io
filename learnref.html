<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Algorithms and Data Structures</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
        }
        h1, h2 {
            color: #2c3e50;
        }
        h3 {
            color: #34495e;
        }
        p, li {
            font-size: 16px;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        .algorithm-title {
            font-weight: bold;
            color: #2980b9;
        }
        a {
            display: inline-block;
            margin-top: 20px;
            text-decoration: none;
            background-color: #2980b9;
            color: white;
            padding: 10px 20px;
            border-radius: 5px;
        }
        a:hover {
            background-color: #3498db;
        }
    </style>
</head>
<body>
    <h1>Algorithms and Data Structures</h1>
    
    <h2>Space and Time Efficiency</h2>
    <p><strong>Importance of Analyzing Algorithmic Complexity</strong></p>
    <p>Analyzing algorithm complexity ensures scalability and efficient handling of larger data. For example, bubble sort has <span class="algorithm-title">O(n²)</span>, while merge sort has <span class="algorithm-title">O(n log n)</span>.</p>
    
    <h3>Orders of Growth</h3>
    <ul>
        <li><strong>O(1)</strong>: Constant time (e.g., array access)</li>
        <li><strong>O(log n)</strong>: Logarithmic time (e.g., binary search)</li>
        <li><strong>O(n)</strong>: Linear time (e.g., iterating through an array)</li>
        <li><strong>O(n²)</strong>: Quadratic time (e.g., bubble sort)</li>
    </ul>

    <h2>Sorting and Searching</h2>

    <h3>Sorting Algorithms</h3>
    
    <p><strong>Bubble Sort:</strong></p>
    <pre class="algorithm-title">
ALGORITHM BubbleSort(A[0..n-1])
    // Sorts a given array using bubble sort
    // Input: An array A[0..n-1] of orderable elements
    // Output: Array A[0...n-1] sorted in ascending order
    for i <- 0 to n - 2 do
        for j <- 0 to n - 2 - i do
            if A[j+1] < A[j]
                swap A[j] and A[j+1]
    </pre>

    <p><strong>Merge Sort:</strong> Efficient divide-and-conquer sorting algorithm with O(n log n) time complexity.</p>
    <pre class="algorithm-title">
ALGORITHM MergeSort(A[0..n-1])
    if n > 1
        copy A[0...|n/2| - 1 ] to B[0...|n/2| - 1]
        copy A[|n/2|... n - 1 ] to C[0......|n/2| - 1]
        MergeSort(B[0...|n/2| - 1])
        MergeSort(C[0......|n/2| - 1])
        Merge(B, C, A)

ALGORITHM Merge(B[0...p-1], C[0...q-1], A[0...p+q-1])
    // Merges two sorted arrays into one sorted array
    // Input: Arrays B[0...p-1] and C[0...q-1] both sorted
    // Output: Sorted array A[0...p+q-1] of the elements of B and C
    i <- 0
    j <- 0
    k <- 0
    while i < p and j < q do
        if B[i] <= C[j]
            A[k] <- B[i]
            i <- i + 1
        else
            A[k] <- C[j]
            j <- j + 1
        k <- k + 1
    if i = p
        copy C[j...q - 1] to A[k...p + q - 1]
    else
        copy B[i...p - 1] to A[k...p + q - 1]
    </pre>
    
    <p><strong>Quick Sort:</strong> Another divide-and-conquer algorithm that selects a pivot and partitions the array.</p>
    <pre class="algorithm-title">
#include <iostream>
#include <vector>
using namespace std;

int partition(vector<int>& A, int l, int r) {
    int p = A[l];
    int i = l;
    int j = r + 1;
    while (true) {
        do {
            i++;
        } while (i <= r && A[i] < p);

        do {
            j--;
        } while (A[j] > p);

        if (i >= j) break;
        swap(A[i], A[j]);
    }
    swap(A[l], A[j]);
    return j;
}

void quickSort(vector<int>& A, int l, int r) {
    if (l < r) {
        int s = partition(A, l, r);
        quickSort(A, l, s - 1);
        quickSort(A, s + 1, r);
    }
}

int main() {
    vector<int> A = {10, 7, 8, 9, 1, 5};
    quickSort(A, 0, A.size() - 1);
    for (int x : A) cout << x << " ";
    return 0;
}
    </pre>

    <h3>Pattern Searching Algorithms</h3>

    <div class="algorithm">
        <h4>1. Brute Force (Naive Approach)</h4>
        <p><strong>Description:</strong> This algorithm checks for the pattern at every possible position in the text. It slides the pattern one by one and checks for a match at each position.</p>
        <p><span class="time-complexity">Time Complexity:</span> O((n-m+1) * m) where n is the length of the text and m is the length of the pattern.</p>
    </div>

    <div class="algorithm">
        <h4>2. Knuth-Morris-Pratt (KMP) Algorithm</h4>
        <p><strong>Description:</strong> KMP preprocesses the pattern to create a partial match table (also known as the "longest prefix suffix" (LPS) array). This allows the algorithm to skip portions of the text without rechecking characters, making it more efficient.</p>
        <p><span class="time-complexity">Time Complexity:</span> O(n + m) where n is the length of the text and m is the length of the pattern.</p>
    </div>

    <div class="algorithm">
        <h4>3. Boyer-Moore Algorithm</h4>
        <p><strong>Description:</strong> Boyer-Moore uses two main heuristics (the Bad Character Heuristic and the Good Suffix Heuristic) to skip sections of the text, moving from right to left. This makes the search faster by potentially skipping large sections of the text.</p>
        <p><span class="time-complexity">Time Complexity:</span> Best case is O(n/m), but in the worst case it is O(n * m). Average performance is often better than KMP.</p>
    </div>

    <div class="algorithm">
        <h4>4. Rabin-Karp Algorithm</h4>
        <p><strong>Description:</strong> Rabin-Karp uses hashing to find any one of a set of pattern strings in a text. It computes a hash for the pattern and a hash for a substring of the text, and compares these hashes. If the hashes match, it then checks for exact matching to avoid false positives.</p>
        <p><span class="time-complexity">Time Complexity:</span> Average case O(n + m), but worst case can be O(n * m) due to hash collisions.</p>
    </div>

    <a href="index.html">Back to Portfolio</a>

    <h2>Algorithm Design Techniques</h2>
    <ul>
        <li><strong>Divide and Conquer:</strong> Breaks problems into smaller sub-problems, solves them independently, and combines results.</li>
        <li><strong>Greedy Algorithms:</strong> Makes the best choice at each step in the hope of finding the global optimum.</li>
        <li><strong>Dynamic Programming:</strong> Solves problems by storing the results of overlapping subproblems to avoid redundant work.</li>
    </ul>

    <h2>Reflections</h2>
    <p>Learning how to break down complex problems into smaller components allows for better problem-solving and optimization. Balancing optimization with simplicity ensures maintainability while delivering efficient solutions. Adapting solutions across different challenges involves choosing the right algorithm for the problem at hand.</p>
</body>
</html>












